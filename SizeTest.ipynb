{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SizeTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_caPmmIRpx6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af00cd5-adeb-4a36-8ccb-ed3403b44e04"
      },
      "source": [
        "!pip install git+https://github.com/finetuneanon/transformers@gpt-neo-dungeon-localattention1\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/finetuneanon/transformers@gpt-neo-dungeon-localattention1\n",
            "  Cloning https://github.com/finetuneanon/transformers (to revision gpt-neo-dungeon-localattention1) to /tmp/pip-req-build-gc44z4o7\n",
            "  Running command git clone -q https://github.com/finetuneanon/transformers /tmp/pip-req-build-gc44z4o7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==4.6.0.dev0 from git+https://github.com/finetuneanon/transformers@gpt-neo-dungeon-localattention1 in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2112854 sha256=b75a1c43ce3f2a2fa87f0c2f530702116400b753ad73bb5e843fa20234ce93f5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2pxd7jn_/wheels/0b/78/87/fcc1dd081713d41c282b4a209e484f9a7dfabb0af14127257e\n",
            "Successfully built transformers\n",
            "Tue Apr 27 16:56:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaqBojtyp0PI"
      },
      "source": [
        "from transformers import GPTNeoForCausalLM, AutoConfig\n",
        "import torch\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "# extra_layers = 26                 # 58 layers\n",
        "# extra_heads = 4                   # 24 heads, 3072 hidden\n",
        "# Parameter count: 6.730748928\n",
        "# extra_layers = 8                  # 40 layers\n",
        "# extra_heads = 8                   # 28 heads, 3584 hidden\n",
        "# Parameter count: 6.354528768\n",
        "extra_layers = 0                  # 32 layers\n",
        "extra_heads = 12                  # 32 heads, 4096 hidden, GPT-3 6.7B\n",
        "# Parameter count: 6.658011136\n",
        "config.attention_layers.extend([\"global\", \"local\"] * (extra_layers // 2))\n",
        "config.attention_types[0][0][1] = 16 + (extra_layers // 2)\n",
        "config.num_layers = 32 + extra_layers\n",
        "config.num_heads = 20 + extra_heads\n",
        "config.hidden_size = 128 * config.num_heads"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNWd9lSryMPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c129da-5cf7-47d5-c30b-48626c208862"
      },
      "source": [
        "model = GPTNeoForCausalLM(config).half().cuda()\n",
        "print(\"Parameter count: \" + str(sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000000))\n",
        "model = model.eval()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter count: 6.658011136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbRNYLNg7FBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ffbe82-c5c4-46f0-c0a8-f5f7e21e1867"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_allocated() / 1024. / 1024.)\n",
        "with torch.no_grad():\n",
        "  for i in range(10):\n",
        "    ids = torch.zeros(1,2048).int().cuda()\n",
        "    output = model(ids)\n",
        "    del ids\n",
        "    del output.loss\n",
        "    del output.logits\n",
        "    del output\n",
        "print(torch.cuda.max_memory_allocated() / 1024. / 1024.)\n",
        "!nvidia-smi | grep MiB"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12763.1640625\n",
            "14687.18798828125\n",
            "| N/A   61C    P0   175W / 250W |  15973MiB / 16280MiB |    100%      Default |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTDjCJV2NRZz"
      },
      "source": [
        "# full checkpoints won't fit into colab system memory, but per-layer checkpoints should work\n",
        "# can be easily converted on a system with more RAM before uploading to google drive\n",
        "import os\n",
        "\n",
        "def save(model):\n",
        "    try: os.mkdir(\"chpt\")\n",
        "    except: pass\n",
        "    checkpoint = {}\n",
        "    for i, x in enumerate(model.state_dict().items()):\n",
        "        checkpoint[x[0]] = f\"chpt/b{i}.pt\"\n",
        "        torch.save(x[1], f\"chpt/b{i}.pt\")\n",
        "    torch.save(checkpoint, f\"chpt/m.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uC5xm9rPBaU",
        "outputId": "00566f82-4e94-4d3d-a4da-c468a24432a9"
      },
      "source": [
        "# save dummy weights\n",
        "print(\"save\", flush=True)\n",
        "save(model)\n",
        "print(\"ok\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save\n",
            "ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tgB70HmOprs",
        "outputId": "df856adb-44a2-4aef-ccf5-64bebdb59d59"
      },
      "source": [
        "# restart runtime and only run the config cell, then this, then the inference cell\n",
        "import collections\n",
        "\n",
        "class Checkpoint(collections.MutableMapping):\n",
        "    def __init__(self):\n",
        "        self.checkpoint = torch.load(\"chpt/m.pt\")\n",
        "    def __len__(self):\n",
        "        return len(self.checkpoint)\n",
        "    def __getitem__(self, key):\n",
        "        return torch.load(self.checkpoint[key])\n",
        "    def __setitem__(self, key, value):\n",
        "        return\n",
        "    def __delitem__(self, key, value):\n",
        "        return\n",
        "    def keys(self):\n",
        "        return self.checkpoint.keys()\n",
        "    def __iter__(self):\n",
        "        for key in self.checkpoint:\n",
        "            yield (key, self.__getitem__(key))\n",
        "    def __copy__(self):\n",
        "        return LazyDict()\n",
        "    def copy(self):\n",
        "        return LazyDict()\n",
        "\n",
        "print(\"load\", flush=True)\n",
        "model = GPTNeoForCausalLM.from_pretrained(pretrained_model_name_or_path=None, config=config, state_dict=Checkpoint())\n",
        "print(\"ok\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ok\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
